{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"27a6ff13dada433202b9ed6acecc68e334b45a42\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pdr.get_data_tiingo('TSLA', api_key=key).reset_index().drop('symbol', axis=1)\n",
    "df = df_original.copy()\n",
    "df['date'] = pd.to_datetime(df['date'].dt.date)\n",
    "df = df[['date', 'close', 'high', 'low', 'open', 'volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-09-14</td>\n",
       "      <td>253.19</td>\n",
       "      <td>254.25</td>\n",
       "      <td>249.67</td>\n",
       "      <td>251.10</td>\n",
       "      <td>2890851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-09-15</td>\n",
       "      <td>253.57</td>\n",
       "      <td>254.60</td>\n",
       "      <td>249.50</td>\n",
       "      <td>252.75</td>\n",
       "      <td>2933466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-09-16</td>\n",
       "      <td>262.25</td>\n",
       "      <td>262.88</td>\n",
       "      <td>252.88</td>\n",
       "      <td>253.04</td>\n",
       "      <td>4417081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>262.07</td>\n",
       "      <td>265.50</td>\n",
       "      <td>260.69</td>\n",
       "      <td>263.96</td>\n",
       "      <td>3585812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-09-18</td>\n",
       "      <td>260.62</td>\n",
       "      <td>263.82</td>\n",
       "      <td>257.50</td>\n",
       "      <td>257.96</td>\n",
       "      <td>3763064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date   close    high     low    open   volume\n",
       "0 2015-09-14  253.19  254.25  249.67  251.10  2890851\n",
       "1 2015-09-15  253.57  254.60  249.50  252.75  2933466\n",
       "2 2015-09-16  262.25  262.88  252.88  253.04  4417081\n",
       "3 2015-09-17  262.07  265.50  260.69  263.96  3585812\n",
       "4 2015-09-18  260.62  263.82  257.50  257.96  3763064"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1259, 6)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = np.where(df['close'].shift(-3) > df['close'], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-09-14</td>\n",
       "      <td>253.19</td>\n",
       "      <td>254.25</td>\n",
       "      <td>249.6700</td>\n",
       "      <td>251.10</td>\n",
       "      <td>2890851</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-09-15</td>\n",
       "      <td>253.57</td>\n",
       "      <td>254.60</td>\n",
       "      <td>249.5000</td>\n",
       "      <td>252.75</td>\n",
       "      <td>2933466</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-09-16</td>\n",
       "      <td>262.25</td>\n",
       "      <td>262.88</td>\n",
       "      <td>252.8800</td>\n",
       "      <td>253.04</td>\n",
       "      <td>4417081</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>262.07</td>\n",
       "      <td>265.50</td>\n",
       "      <td>260.6900</td>\n",
       "      <td>263.96</td>\n",
       "      <td>3585812</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-09-18</td>\n",
       "      <td>260.62</td>\n",
       "      <td>263.82</td>\n",
       "      <td>257.5000</td>\n",
       "      <td>257.96</td>\n",
       "      <td>3763064</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-09-21</td>\n",
       "      <td>264.20</td>\n",
       "      <td>271.57</td>\n",
       "      <td>255.8000</td>\n",
       "      <td>263.98</td>\n",
       "      <td>6120155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-09-22</td>\n",
       "      <td>260.94</td>\n",
       "      <td>262.65</td>\n",
       "      <td>255.8700</td>\n",
       "      <td>259.03</td>\n",
       "      <td>3664353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015-09-23</td>\n",
       "      <td>261.06</td>\n",
       "      <td>262.08</td>\n",
       "      <td>257.5838</td>\n",
       "      <td>261.95</td>\n",
       "      <td>2600778</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-09-24</td>\n",
       "      <td>263.12</td>\n",
       "      <td>263.45</td>\n",
       "      <td>256.2100</td>\n",
       "      <td>259.53</td>\n",
       "      <td>3448191</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015-09-25</td>\n",
       "      <td>256.91</td>\n",
       "      <td>266.91</td>\n",
       "      <td>256.1500</td>\n",
       "      <td>266.61</td>\n",
       "      <td>3773392</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2015-09-28</td>\n",
       "      <td>248.43</td>\n",
       "      <td>259.79</td>\n",
       "      <td>246.6100</td>\n",
       "      <td>257.35</td>\n",
       "      <td>4881802</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2015-09-29</td>\n",
       "      <td>246.65</td>\n",
       "      <td>254.73</td>\n",
       "      <td>245.4600</td>\n",
       "      <td>250.46</td>\n",
       "      <td>3703154</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2015-09-30</td>\n",
       "      <td>248.40</td>\n",
       "      <td>252.40</td>\n",
       "      <td>242.3400</td>\n",
       "      <td>252.00</td>\n",
       "      <td>4879816</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>239.88</td>\n",
       "      <td>248.50</td>\n",
       "      <td>237.1300</td>\n",
       "      <td>247.51</td>\n",
       "      <td>4572964</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2015-10-02</td>\n",
       "      <td>247.57</td>\n",
       "      <td>247.70</td>\n",
       "      <td>234.9300</td>\n",
       "      <td>235.60</td>\n",
       "      <td>4423982</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2015-10-05</td>\n",
       "      <td>246.15</td>\n",
       "      <td>249.84</td>\n",
       "      <td>244.1300</td>\n",
       "      <td>248.84</td>\n",
       "      <td>3689865</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2015-10-06</td>\n",
       "      <td>241.46</td>\n",
       "      <td>243.03</td>\n",
       "      <td>235.5800</td>\n",
       "      <td>240.00</td>\n",
       "      <td>5235897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2015-10-07</td>\n",
       "      <td>231.96</td>\n",
       "      <td>237.70</td>\n",
       "      <td>229.1200</td>\n",
       "      <td>236.63</td>\n",
       "      <td>6813959</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2015-10-08</td>\n",
       "      <td>226.72</td>\n",
       "      <td>230.72</td>\n",
       "      <td>221.3100</td>\n",
       "      <td>230.08</td>\n",
       "      <td>6133216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2015-10-09</td>\n",
       "      <td>220.69</td>\n",
       "      <td>224.37</td>\n",
       "      <td>218.3600</td>\n",
       "      <td>220.93</td>\n",
       "      <td>6158370</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   close    high       low    open   volume  label\n",
       "0  2015-09-14  253.19  254.25  249.6700  251.10  2890851      1\n",
       "1  2015-09-15  253.57  254.60  249.5000  252.75  2933466      1\n",
       "2  2015-09-16  262.25  262.88  252.8800  253.04  4417081      0\n",
       "3  2015-09-17  262.07  265.50  260.6900  263.96  3585812      1\n",
       "4  2015-09-18  260.62  263.82  257.5000  257.96  3763064      1\n",
       "5  2015-09-21  264.20  271.57  255.8000  263.98  6120155      0\n",
       "6  2015-09-22  260.94  262.65  255.8700  259.03  3664353      1\n",
       "7  2015-09-23  261.06  262.08  257.5838  261.95  2600778      0\n",
       "8  2015-09-24  263.12  263.45  256.2100  259.53  3448191      0\n",
       "9  2015-09-25  256.91  266.91  256.1500  266.61  3773392      0\n",
       "10 2015-09-28  248.43  259.79  246.6100  257.35  4881802      0\n",
       "11 2015-09-29  246.65  254.73  245.4600  250.46  3703154      0\n",
       "12 2015-09-30  248.40  252.40  242.3400  252.00  4879816      0\n",
       "13 2015-10-01  239.88  248.50  237.1300  247.51  4572964      1\n",
       "14 2015-10-02  247.57  247.70  234.9300  235.60  4423982      0\n",
       "15 2015-10-05  246.15  249.84  244.1300  248.84  3689865      0\n",
       "16 2015-10-06  241.46  243.03  235.5800  240.00  5235897      0\n",
       "17 2015-10-07  231.96  237.70  229.1200  236.63  6813959      0\n",
       "18 2015-10-08  226.72  230.72  221.3100  230.08  6133216      0\n",
       "19 2015-10-09  220.69  224.37  218.3600  220.93  6158370      0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sma5']  = df['close'].rolling(5).mean()\n",
    "df['sma10']  = df['close'].rolling(10).mean()\n",
    "df['sma50']  = df['close'].rolling(50).mean()\n",
    "\n",
    "df['ema5'] = df['close'].ewm(span=5, min_periods=0, adjust=False, ignore_na=False).mean()\n",
    "df['ema10'] = df['close'].ewm(span=10, min_periods=0, adjust=False, ignore_na=False).mean()\n",
    "df['ema50'] = df['close'].ewm(span=50, min_periods=0, adjust=False, ignore_na=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wwma(values, n):\n",
    "    \"\"\"\n",
    "     J. Welles Wilder's EMA \n",
    "    \"\"\"\n",
    "    return values.ewm(alpha=1/n, adjust=False, min_periods=n).mean()\n",
    "\n",
    "\n",
    "def atr(df, n=14):\n",
    "    data = df.copy()\n",
    "    high = data['high']\n",
    "    low = data['low']\n",
    "    close = data['close']\n",
    "    data['tr0'] = abs(high - low)\n",
    "    data['tr1'] = abs(high - close.shift())\n",
    "    data['tr2'] = abs(low - close.shift())\n",
    "    tr = data[['tr0', 'tr1', 'tr2']].max(axis=1)\n",
    "    atr = wwma(tr, n)\n",
    "    return atr\n",
    "\n",
    "\n",
    "df['ATRP'] = atr(df)/df['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>label</th>\n",
       "      <th>sma5</th>\n",
       "      <th>sma10</th>\n",
       "      <th>sma50</th>\n",
       "      <th>ema5</th>\n",
       "      <th>ema10</th>\n",
       "      <th>ema50</th>\n",
       "      <th>ATRP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-09-14</td>\n",
       "      <td>253.19</td>\n",
       "      <td>254.25</td>\n",
       "      <td>249.6700</td>\n",
       "      <td>251.10</td>\n",
       "      <td>2890851</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>253.190000</td>\n",
       "      <td>253.190000</td>\n",
       "      <td>253.190000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-09-15</td>\n",
       "      <td>253.57</td>\n",
       "      <td>254.60</td>\n",
       "      <td>249.5000</td>\n",
       "      <td>252.75</td>\n",
       "      <td>2933466</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>253.316667</td>\n",
       "      <td>253.259091</td>\n",
       "      <td>253.204902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-09-16</td>\n",
       "      <td>262.25</td>\n",
       "      <td>262.88</td>\n",
       "      <td>252.8800</td>\n",
       "      <td>253.04</td>\n",
       "      <td>4417081</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>256.294444</td>\n",
       "      <td>254.893802</td>\n",
       "      <td>253.559612</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>262.07</td>\n",
       "      <td>265.50</td>\n",
       "      <td>260.6900</td>\n",
       "      <td>263.96</td>\n",
       "      <td>3585812</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>258.219630</td>\n",
       "      <td>256.198565</td>\n",
       "      <td>253.893352</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-09-18</td>\n",
       "      <td>260.62</td>\n",
       "      <td>263.82</td>\n",
       "      <td>257.5000</td>\n",
       "      <td>257.96</td>\n",
       "      <td>3763064</td>\n",
       "      <td>1</td>\n",
       "      <td>258.340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>259.019753</td>\n",
       "      <td>257.002462</td>\n",
       "      <td>254.157143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>2020-09-04</td>\n",
       "      <td>418.32</td>\n",
       "      <td>428.00</td>\n",
       "      <td>372.0201</td>\n",
       "      <td>402.81</td>\n",
       "      <td>110321885</td>\n",
       "      <td>0</td>\n",
       "      <td>449.212</td>\n",
       "      <td>1288.892</td>\n",
       "      <td>1454.6546</td>\n",
       "      <td>657.641991</td>\n",
       "      <td>1020.182115</td>\n",
       "      <td>1356.210403</td>\n",
       "      <td>0.440290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>2020-09-08</td>\n",
       "      <td>330.21</td>\n",
       "      <td>368.74</td>\n",
       "      <td>329.8800</td>\n",
       "      <td>356.00</td>\n",
       "      <td>115465691</td>\n",
       "      <td>1</td>\n",
       "      <td>415.590</td>\n",
       "      <td>1120.493</td>\n",
       "      <td>1442.0640</td>\n",
       "      <td>548.497994</td>\n",
       "      <td>894.732640</td>\n",
       "      <td>1315.975093</td>\n",
       "      <td>0.537062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>366.28</td>\n",
       "      <td>369.00</td>\n",
       "      <td>341.5100</td>\n",
       "      <td>356.60</td>\n",
       "      <td>79465769</td>\n",
       "      <td>1</td>\n",
       "      <td>393.836</td>\n",
       "      <td>954.787</td>\n",
       "      <td>1429.2026</td>\n",
       "      <td>487.758662</td>\n",
       "      <td>798.650342</td>\n",
       "      <td>1278.732148</td>\n",
       "      <td>0.457155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>371.34</td>\n",
       "      <td>398.99</td>\n",
       "      <td>360.5600</td>\n",
       "      <td>386.21</td>\n",
       "      <td>84930608</td>\n",
       "      <td>0</td>\n",
       "      <td>378.630</td>\n",
       "      <td>776.604</td>\n",
       "      <td>1415.0332</td>\n",
       "      <td>448.952442</td>\n",
       "      <td>720.957552</td>\n",
       "      <td>1243.148142</td>\n",
       "      <td>0.426109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>2020-09-11</td>\n",
       "      <td>372.72</td>\n",
       "      <td>382.50</td>\n",
       "      <td>360.5000</td>\n",
       "      <td>381.94</td>\n",
       "      <td>60717459</td>\n",
       "      <td>0</td>\n",
       "      <td>371.774</td>\n",
       "      <td>590.001</td>\n",
       "      <td>1400.0950</td>\n",
       "      <td>423.541628</td>\n",
       "      <td>657.641634</td>\n",
       "      <td>1209.013705</td>\n",
       "      <td>0.398423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1259 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date   close    high       low    open     volume  label     sma5  \\\n",
       "0    2015-09-14  253.19  254.25  249.6700  251.10    2890851      1      NaN   \n",
       "1    2015-09-15  253.57  254.60  249.5000  252.75    2933466      1      NaN   \n",
       "2    2015-09-16  262.25  262.88  252.8800  253.04    4417081      0      NaN   \n",
       "3    2015-09-17  262.07  265.50  260.6900  263.96    3585812      1      NaN   \n",
       "4    2015-09-18  260.62  263.82  257.5000  257.96    3763064      1  258.340   \n",
       "...         ...     ...     ...       ...     ...        ...    ...      ...   \n",
       "1254 2020-09-04  418.32  428.00  372.0201  402.81  110321885      0  449.212   \n",
       "1255 2020-09-08  330.21  368.74  329.8800  356.00  115465691      1  415.590   \n",
       "1256 2020-09-09  366.28  369.00  341.5100  356.60   79465769      1  393.836   \n",
       "1257 2020-09-10  371.34  398.99  360.5600  386.21   84930608      0  378.630   \n",
       "1258 2020-09-11  372.72  382.50  360.5000  381.94   60717459      0  371.774   \n",
       "\n",
       "         sma10      sma50        ema5        ema10        ema50      ATRP  \n",
       "0          NaN        NaN  253.190000   253.190000   253.190000       NaN  \n",
       "1          NaN        NaN  253.316667   253.259091   253.204902       NaN  \n",
       "2          NaN        NaN  256.294444   254.893802   253.559612       NaN  \n",
       "3          NaN        NaN  258.219630   256.198565   253.893352       NaN  \n",
       "4          NaN        NaN  259.019753   257.002462   254.157143       NaN  \n",
       "...        ...        ...         ...          ...          ...       ...  \n",
       "1254  1288.892  1454.6546  657.641991  1020.182115  1356.210403  0.440290  \n",
       "1255  1120.493  1442.0640  548.497994   894.732640  1315.975093  0.537062  \n",
       "1256   954.787  1429.2026  487.758662   798.650342  1278.732148  0.457155  \n",
       "1257   776.604  1415.0332  448.952442   720.957552  1243.148142  0.426109  \n",
       "1258   590.001  1400.0950  423.541628   657.641634  1209.013705  0.398423  \n",
       "\n",
       "[1259 rows x 14 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('C:/Users/Jesse/Downloads/abcnews-date-text.csv')\n",
    "news['publish_date'] = pd.to_datetime(news['publish_date'], format='%Y%m%d')\n",
    "news = news.groupby('publish_date')['headline_text'].sum()\n",
    "news = pd.DataFrame(news).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news.loc[news.publish_date >= pd.to_datetime('2015-09-14')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4582    2015 live export season wa32 jobs go nickel mi...\n",
       "4583    7 things know malcolm turnbullabbott dumping g...\n",
       "4584    150 people attended heated meeting bendigo40 y...\n",
       "4585    10 year anniversary mcgrath foundation85 kille...\n",
       "4586    2.3 million indians apply 368 low level govern...\n",
       "                              ...                        \n",
       "6147    abc pic week best 2019alexei navalny office ra...\n",
       "6148    acosia red elk demonstrating jingle dress danc...\n",
       "6149    alcohol addiction made drug dealer sent prison...\n",
       "6150    aerial firefighting bundooraafrican swine feve...\n",
       "6151    abc reporter liv casben seek shelter batemans ...\n",
       "Name: headline_text, Length: 1570, dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "news['headline_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "    \n",
    "def get_polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def get_SIA(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['subjectivity'] = news['headline_text'].apply(get_subjectivity)\n",
    "news['polarity'] = news['headline_text'].apply(get_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound = []\n",
    "neg = []\n",
    "pos = []\n",
    "neu = []\n",
    "\n",
    "\n",
    "for text in news['headline_text']:\n",
    "    SIA = get_SIA(text)\n",
    "    \n",
    "    compound.append(SIA['compound'])\n",
    "    neg.append(SIA['neg'])\n",
    "    pos.append(SIA['pos'])\n",
    "    neu.append(SIA['neu'])\n",
    "\n",
    "\n",
    "news['compound'] = compound\n",
    "news['neg'] = neg\n",
    "news['pos'] = pos\n",
    "news['neu'] = neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.rename({'publish_date': 'date'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = df.merge(news, how='inner', on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1034,), (1034, 21))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged['label'].shape, merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.drop(['date', 'headline_text'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = merged.drop('label', axis=1)\n",
    "targets = merged['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make the actual data which will go into the neural network\n",
    "T = 10 # the number of time steps to look at to make a prediction for the next day\n",
    "D = input_data.shape[1]\n",
    "N = len(input_data) - T # (e.g. if T=10 and you have 11 data points then you'd only have 1 sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# normalize the inputs\n",
    "Ntrain = len(input_data) * 2 // 3\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(input_data[:Ntrain + T - 1])\n",
    "input_data = scaler.transform(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup X_train and Y_train\n",
    "X_train = np.zeros((Ntrain, T, D))\n",
    "Y_train = np.zeros(Ntrain)\n",
    "\n",
    "for t in range(Ntrain):\n",
    "    X_train[t, :, :] = input_data[t:t+T]\n",
    "    Y_train[t] = (targets[t+T] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup X_test and Y_test\n",
    "X_test = np.zeros((N - Ntrain, T, D))\n",
    "Y_test = np.zeros(N - Ntrain)\n",
    "\n",
    "for u in range(N - Ntrain):\n",
    "    # u counts from 0...(N - Ntrain)\n",
    "    # t counts from Ntrain...N\n",
    "    t = u + Ntrain\n",
    "    X_test[u, :, :] = input_data[t:t+T]\n",
    "    Y_test[u] = (targets[t+T] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((335, 10, 18), (335,))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((689, 10, 18), (689,))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 689 samples, validate on 335 samples\n",
      "Epoch 1/100\n",
      "689/689 [==============================] - 1s 2ms/sample - loss: 0.6967 - acc: 0.4935 - val_loss: 0.6947 - val_acc: 0.4746\n",
      "Epoch 2/100\n",
      "689/689 [==============================] - 0s 331us/sample - loss: 0.6909 - acc: 0.5269 - val_loss: 0.6977 - val_acc: 0.4955\n",
      "Epoch 3/100\n",
      "689/689 [==============================] - 0s 328us/sample - loss: 0.6936 - acc: 0.5138 - val_loss: 0.6902 - val_acc: 0.5731\n",
      "Epoch 4/100\n",
      "689/689 [==============================] - 0s 327us/sample - loss: 0.6789 - acc: 0.5602 - val_loss: 0.7278 - val_acc: 0.4806\n",
      "Epoch 5/100\n",
      "689/689 [==============================] - 0s 332us/sample - loss: 0.6793 - acc: 0.5922 - val_loss: 0.7695 - val_acc: 0.4478\n",
      "Epoch 6/100\n",
      "689/689 [==============================] - 0s 329us/sample - loss: 0.6709 - acc: 0.5878 - val_loss: 0.7248 - val_acc: 0.5313\n",
      "Epoch 7/100\n",
      "689/689 [==============================] - 0s 327us/sample - loss: 0.6708 - acc: 0.6023 - val_loss: 0.7347 - val_acc: 0.5463\n",
      "Epoch 8/100\n",
      "689/689 [==============================] - 0s 327us/sample - loss: 0.6574 - acc: 0.6197 - val_loss: 0.7601 - val_acc: 0.4627\n",
      "Epoch 9/100\n",
      "689/689 [==============================] - 0s 338us/sample - loss: 0.6427 - acc: 0.6401 - val_loss: 0.8069 - val_acc: 0.5104\n",
      "Epoch 10/100\n",
      "689/689 [==============================] - 0s 335us/sample - loss: 0.6254 - acc: 0.6662 - val_loss: 0.8280 - val_acc: 0.5522\n",
      "Epoch 11/100\n",
      "689/689 [==============================] - 0s 335us/sample - loss: 0.6250 - acc: 0.6633 - val_loss: 0.8200 - val_acc: 0.4955\n",
      "Epoch 12/100\n",
      "689/689 [==============================] - 0s 329us/sample - loss: 0.6058 - acc: 0.6952 - val_loss: 0.8984 - val_acc: 0.4806\n",
      "Epoch 13/100\n",
      "689/689 [==============================] - 0s 329us/sample - loss: 0.5865 - acc: 0.6923 - val_loss: 0.8923 - val_acc: 0.5313\n",
      "Epoch 14/100\n",
      "689/689 [==============================] - 0s 328us/sample - loss: 0.5640 - acc: 0.7170 - val_loss: 0.9376 - val_acc: 0.4806\n",
      "Epoch 15/100\n",
      "689/689 [==============================] - 0s 328us/sample - loss: 0.5470 - acc: 0.7228 - val_loss: 1.0375 - val_acc: 0.4597\n",
      "Epoch 16/100\n",
      "689/689 [==============================] - 0s 338us/sample - loss: 0.5758 - acc: 0.6967 - val_loss: 0.8989 - val_acc: 0.4866\n",
      "Epoch 17/100\n",
      "689/689 [==============================] - 0s 328us/sample - loss: 0.5471 - acc: 0.7170 - val_loss: 1.0586 - val_acc: 0.4687\n",
      "Epoch 18/100\n",
      "689/689 [==============================] - 0s 329us/sample - loss: 0.5085 - acc: 0.7576 - val_loss: 1.0320 - val_acc: 0.5194\n",
      "Epoch 19/100\n",
      "689/689 [==============================] - 0s 325us/sample - loss: 0.4773 - acc: 0.7634 - val_loss: 1.1301 - val_acc: 0.5045\n",
      "Epoch 20/100\n",
      "689/689 [==============================] - 0s 324us/sample - loss: 0.4924 - acc: 0.7678 - val_loss: 1.0190 - val_acc: 0.4985\n",
      "Epoch 21/100\n",
      "689/689 [==============================] - 0s 327us/sample - loss: 0.4707 - acc: 0.7765 - val_loss: 1.2032 - val_acc: 0.4388\n",
      "Epoch 22/100\n",
      "689/689 [==============================] - 0s 325us/sample - loss: 0.4433 - acc: 0.7910 - val_loss: 1.2895 - val_acc: 0.4955\n",
      "Epoch 23/100\n",
      "689/689 [==============================] - 0s 327us/sample - loss: 0.3886 - acc: 0.8215 - val_loss: 1.5388 - val_acc: 0.4925\n",
      "Epoch 24/100\n",
      "689/689 [==============================] - 0s 322us/sample - loss: 0.4033 - acc: 0.8215 - val_loss: 1.5241 - val_acc: 0.4537\n",
      "Epoch 25/100\n",
      "689/689 [==============================] - 0s 321us/sample - loss: 0.4087 - acc: 0.8041 - val_loss: 1.4194 - val_acc: 0.5164\n",
      "Epoch 26/100\n",
      "689/689 [==============================] - 0s 325us/sample - loss: 0.3604 - acc: 0.8374 - val_loss: 1.5065 - val_acc: 0.4716\n",
      "Epoch 27/100\n",
      "689/689 [==============================] - 0s 325us/sample - loss: 0.3346 - acc: 0.8476 - val_loss: 1.7695 - val_acc: 0.4836\n",
      "Epoch 28/100\n",
      "689/689 [==============================] - 0s 321us/sample - loss: 0.3190 - acc: 0.8505 - val_loss: 1.9148 - val_acc: 0.4836\n",
      "Epoch 29/100\n",
      "689/689 [==============================] - 0s 322us/sample - loss: 0.3740 - acc: 0.8215 - val_loss: 1.3761 - val_acc: 0.5104\n",
      "Epoch 30/100\n",
      "689/689 [==============================] - 0s 322us/sample - loss: 0.3168 - acc: 0.8737 - val_loss: 2.2475 - val_acc: 0.4985\n",
      "Epoch 31/100\n",
      "689/689 [==============================] - 0s 327us/sample - loss: 0.2950 - acc: 0.8708 - val_loss: 2.0196 - val_acc: 0.5045\n",
      "Epoch 32/100\n",
      "689/689 [==============================] - 0s 324us/sample - loss: 0.3120 - acc: 0.8592 - val_loss: 2.1453 - val_acc: 0.4955\n",
      "Epoch 33/100\n",
      "689/689 [==============================] - 0s 328us/sample - loss: 0.2656 - acc: 0.8824 - val_loss: 2.2558 - val_acc: 0.4746\n",
      "Epoch 34/100\n",
      "689/689 [==============================] - 0s 322us/sample - loss: 0.2840 - acc: 0.8810 - val_loss: 2.2536 - val_acc: 0.4627\n",
      "Epoch 35/100\n",
      "689/689 [==============================] - 0s 327us/sample - loss: 0.3147 - acc: 0.8636 - val_loss: 1.9372 - val_acc: 0.4896\n",
      "Epoch 36/100\n",
      "689/689 [==============================] - 0s 325us/sample - loss: 0.2412 - acc: 0.8999 - val_loss: 2.5597 - val_acc: 0.4567\n",
      "Epoch 37/100\n",
      "689/689 [==============================] - 0s 325us/sample - loss: 0.2075 - acc: 0.9057 - val_loss: 3.0738 - val_acc: 0.4925\n",
      "Epoch 38/100\n",
      "689/689 [==============================] - 0s 322us/sample - loss: 0.2113 - acc: 0.9129 - val_loss: 2.8391 - val_acc: 0.4448\n",
      "Epoch 39/100\n",
      "689/689 [==============================] - 0s 324us/sample - loss: 0.2257 - acc: 0.8955 - val_loss: 3.1980 - val_acc: 0.5045\n",
      "Epoch 40/100\n",
      "689/689 [==============================] - 0s 325us/sample - loss: 0.2194 - acc: 0.9086 - val_loss: 3.1270 - val_acc: 0.4776\n",
      "Epoch 41/100\n",
      "689/689 [==============================] - 0s 327us/sample - loss: 0.2165 - acc: 0.9115 - val_loss: 3.0575 - val_acc: 0.4746\n",
      "Epoch 42/100\n",
      "689/689 [==============================] - 0s 329us/sample - loss: 0.2474 - acc: 0.8999 - val_loss: 2.7577 - val_acc: 0.4776\n",
      "Epoch 43/100\n",
      "689/689 [==============================] - 0s 331us/sample - loss: 0.2035 - acc: 0.9158 - val_loss: 3.1149 - val_acc: 0.4806\n",
      "Epoch 44/100\n",
      "689/689 [==============================] - 0s 331us/sample - loss: 0.1744 - acc: 0.9245 - val_loss: 3.2669 - val_acc: 0.4925\n",
      "Epoch 45/100\n",
      "689/689 [==============================] - 0s 325us/sample - loss: 0.1529 - acc: 0.9347 - val_loss: 3.2026 - val_acc: 0.4836\n",
      "Epoch 46/100\n",
      "689/689 [==============================] - 0s 324us/sample - loss: 0.1427 - acc: 0.9390 - val_loss: 4.0287 - val_acc: 0.4746\n",
      "Epoch 47/100\n",
      "689/689 [==============================] - 0s 327us/sample - loss: 0.1624 - acc: 0.9347 - val_loss: 3.7497 - val_acc: 0.4866\n",
      "Epoch 48/100\n",
      "689/689 [==============================] - 0s 325us/sample - loss: 0.1579 - acc: 0.9390 - val_loss: 3.8016 - val_acc: 0.5015\n",
      "Epoch 49/100\n",
      "689/689 [==============================] - 0s 322us/sample - loss: 0.1211 - acc: 0.9536 - val_loss: 4.2556 - val_acc: 0.4955\n",
      "Epoch 50/100\n",
      "689/689 [==============================] - 0s 324us/sample - loss: 0.1413 - acc: 0.9419 - val_loss: 4.2435 - val_acc: 0.4746\n",
      "Epoch 51/100\n",
      "689/689 [==============================] - 0s 327us/sample - loss: 0.1272 - acc: 0.9492 - val_loss: 3.9760 - val_acc: 0.4657\n",
      "Epoch 52/100\n",
      "689/689 [==============================] - 0s 322us/sample - loss: 0.0953 - acc: 0.9608 - val_loss: 4.6877 - val_acc: 0.4776\n",
      "Epoch 53/100\n",
      "689/689 [==============================] - 0s 337us/sample - loss: 0.1284 - acc: 0.9550 - val_loss: 3.8538 - val_acc: 0.4507\n",
      "Epoch 54/100\n",
      "689/689 [==============================] - 0s 324us/sample - loss: 0.1163 - acc: 0.9652 - val_loss: 4.4048 - val_acc: 0.4955\n",
      "Epoch 55/100\n",
      "689/689 [==============================] - 0s 324us/sample - loss: 0.0793 - acc: 0.9710 - val_loss: 4.7640 - val_acc: 0.4657\n",
      "Epoch 56/100\n",
      "689/689 [==============================] - 0s 324us/sample - loss: 0.1202 - acc: 0.9536 - val_loss: 4.4316 - val_acc: 0.4896\n",
      "Epoch 57/100\n",
      "689/689 [==============================] - 0s 325us/sample - loss: 0.0948 - acc: 0.9623 - val_loss: 4.5223 - val_acc: 0.4657\n",
      "Epoch 58/100\n",
      "689/689 [==============================] - 0s 325us/sample - loss: 0.0884 - acc: 0.9710 - val_loss: 4.4494 - val_acc: 0.4776\n",
      "Epoch 59/100\n",
      "689/689 [==============================] - 0s 325us/sample - loss: 0.1301 - acc: 0.9594 - val_loss: 4.3285 - val_acc: 0.4716\n",
      "Epoch 60/100\n",
      "689/689 [==============================] - 0s 324us/sample - loss: 0.1035 - acc: 0.9608 - val_loss: 4.2354 - val_acc: 0.4955\n",
      "Epoch 61/100\n",
      "689/689 [==============================] - 0s 321us/sample - loss: 0.0667 - acc: 0.9710 - val_loss: 5.0829 - val_acc: 0.4806\n",
      "Epoch 62/100\n",
      "689/689 [==============================] - 0s 325us/sample - loss: 0.1051 - acc: 0.9666 - val_loss: 4.5598 - val_acc: 0.4955\n",
      "Epoch 63/100\n",
      "689/689 [==============================] - 0s 325us/sample - loss: 0.0653 - acc: 0.9782 - val_loss: 4.6390 - val_acc: 0.5104\n",
      "Epoch 64/100\n",
      "689/689 [==============================] - 0s 324us/sample - loss: 0.0439 - acc: 0.9826 - val_loss: 5.0361 - val_acc: 0.4866\n",
      "Epoch 65/100\n",
      "689/689 [==============================] - 0s 324us/sample - loss: 0.0445 - acc: 0.9840 - val_loss: 5.6116 - val_acc: 0.4836\n",
      "Epoch 66/100\n",
      "689/689 [==============================] - 0s 340us/sample - loss: 0.0569 - acc: 0.9811 - val_loss: 4.9251 - val_acc: 0.4925\n",
      "Epoch 67/100\n",
      "689/689 [==============================] - 0s 325us/sample - loss: 0.1039 - acc: 0.9681 - val_loss: 4.6965 - val_acc: 0.4985\n",
      "Epoch 68/100\n",
      "689/689 [==============================] - 0s 327us/sample - loss: 0.1257 - acc: 0.9623 - val_loss: 3.3337 - val_acc: 0.4896\n",
      "Epoch 69/100\n",
      "689/689 [==============================] - 0s 324us/sample - loss: 0.0817 - acc: 0.9695 - val_loss: 3.8270 - val_acc: 0.5015\n",
      "Epoch 70/100\n",
      "689/689 [==============================] - 0s 321us/sample - loss: 0.0364 - acc: 0.9884 - val_loss: 4.9407 - val_acc: 0.4776\n",
      "Epoch 71/100\n",
      "689/689 [==============================] - 0s 327us/sample - loss: 0.0550 - acc: 0.9855 - val_loss: 5.0354 - val_acc: 0.4776\n",
      "Epoch 72/100\n",
      "689/689 [==============================] - 0s 325us/sample - loss: 0.0605 - acc: 0.9739 - val_loss: 4.8485 - val_acc: 0.4776\n",
      "Epoch 73/100\n",
      "689/689 [==============================] - 0s 322us/sample - loss: 0.0382 - acc: 0.9884 - val_loss: 4.9775 - val_acc: 0.4597\n",
      "Epoch 74/100\n",
      "689/689 [==============================] - 0s 328us/sample - loss: 0.0634 - acc: 0.9811 - val_loss: 5.0596 - val_acc: 0.4657\n",
      "Epoch 75/100\n",
      "689/689 [==============================] - 0s 327us/sample - loss: 0.0476 - acc: 0.9811 - val_loss: 4.9400 - val_acc: 0.4627\n",
      "Epoch 76/100\n",
      "689/689 [==============================] - 0s 324us/sample - loss: 0.0503 - acc: 0.9826 - val_loss: 5.1021 - val_acc: 0.4806\n",
      "Epoch 77/100\n",
      "689/689 [==============================] - 0s 327us/sample - loss: 0.0572 - acc: 0.9811 - val_loss: 5.2292 - val_acc: 0.4746\n",
      "Epoch 78/100\n",
      "689/689 [==============================] - 0s 327us/sample - loss: 0.1218 - acc: 0.9710 - val_loss: 3.7531 - val_acc: 0.4896\n",
      "Epoch 79/100\n",
      "689/689 [==============================] - 0s 329us/sample - loss: 0.0489 - acc: 0.9869 - val_loss: 4.2201 - val_acc: 0.4836\n",
      "Epoch 80/100\n",
      "689/689 [==============================] - 0s 325us/sample - loss: 0.0223 - acc: 0.9942 - val_loss: 4.9780 - val_acc: 0.4925\n",
      "Epoch 81/100\n",
      "689/689 [==============================] - 0s 332us/sample - loss: 0.0857 - acc: 0.9898 - val_loss: 5.4962 - val_acc: 0.4776\n",
      "Epoch 82/100\n",
      "689/689 [==============================] - 0s 328us/sample - loss: 0.0212 - acc: 0.9985 - val_loss: 5.2212 - val_acc: 0.4597\n",
      "Epoch 83/100\n",
      "689/689 [==============================] - 0s 340us/sample - loss: 0.0321 - acc: 0.9913 - val_loss: 5.1379 - val_acc: 0.4597\n",
      "Epoch 84/100\n",
      "689/689 [==============================] - 0s 327us/sample - loss: 0.0334 - acc: 0.9898 - val_loss: 5.0036 - val_acc: 0.4866\n",
      "Epoch 85/100\n",
      "689/689 [==============================] - 0s 322us/sample - loss: 0.0513 - acc: 0.9826 - val_loss: 5.0707 - val_acc: 0.4746\n",
      "Epoch 86/100\n",
      "689/689 [==============================] - 0s 331us/sample - loss: 0.0488 - acc: 0.9811 - val_loss: 4.6606 - val_acc: 0.4716\n",
      "Epoch 87/100\n",
      "689/689 [==============================] - 0s 328us/sample - loss: 0.0348 - acc: 0.9898 - val_loss: 5.0324 - val_acc: 0.4806\n",
      "Epoch 88/100\n",
      "689/689 [==============================] - 0s 334us/sample - loss: 0.0564 - acc: 0.9811 - val_loss: 4.8815 - val_acc: 0.4716\n",
      "Epoch 89/100\n",
      "689/689 [==============================] - 0s 329us/sample - loss: 0.0242 - acc: 0.9927 - val_loss: 4.9768 - val_acc: 0.4687\n",
      "Epoch 90/100\n",
      "689/689 [==============================] - 0s 327us/sample - loss: 0.0600 - acc: 0.9869 - val_loss: 4.6073 - val_acc: 0.4746\n",
      "Epoch 91/100\n",
      "689/689 [==============================] - 0s 338us/sample - loss: 0.0408 - acc: 0.9898 - val_loss: 4.7483 - val_acc: 0.4687\n",
      "Epoch 92/100\n",
      "689/689 [==============================] - 0s 329us/sample - loss: 0.0420 - acc: 0.9898 - val_loss: 4.6622 - val_acc: 0.4866\n",
      "Epoch 93/100\n",
      "689/689 [==============================] - 0s 332us/sample - loss: 0.0335 - acc: 0.9898 - val_loss: 4.8591 - val_acc: 0.4925\n",
      "Epoch 94/100\n",
      "689/689 [==============================] - 0s 331us/sample - loss: 0.1043 - acc: 0.9739 - val_loss: 4.0430 - val_acc: 0.4776\n",
      "Epoch 95/100\n",
      "689/689 [==============================] - 0s 331us/sample - loss: 0.0589 - acc: 0.9797 - val_loss: 4.1007 - val_acc: 0.4627\n",
      "Epoch 96/100\n",
      "689/689 [==============================] - 0s 329us/sample - loss: 0.0253 - acc: 0.9927 - val_loss: 4.8944 - val_acc: 0.4687\n",
      "Epoch 97/100\n",
      "689/689 [==============================] - 0s 328us/sample - loss: 0.0207 - acc: 0.9971 - val_loss: 5.3375 - val_acc: 0.4746\n",
      "Epoch 98/100\n",
      "689/689 [==============================] - 0s 337us/sample - loss: 0.0116 - acc: 0.9956 - val_loss: 5.5645 - val_acc: 0.4716\n",
      "Epoch 99/100\n",
      "689/689 [==============================] - 0s 331us/sample - loss: 0.0196 - acc: 0.9942 - val_loss: 5.3103 - val_acc: 0.4836\n",
      "Epoch 100/100\n",
      "689/689 [==============================] - 0s 329us/sample - loss: 0.0130 - acc: 0.9971 - val_loss: 5.4033 - val_acc: 0.4896\n"
     ]
    }
   ],
   "source": [
    "# More imports\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, SimpleRNN, Dense, GlobalMaxPool1D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# make the RNN\n",
    "i = Input(shape=(T, D))\n",
    "x = LSTM(50)(i)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(i, x)\n",
    "model.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer=Adam(lr=0.001),\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "\n",
    "# train the RNN\n",
    "r = model.fit(\n",
    "  X_train, Y_train,\n",
    "  batch_size=32,\n",
    "  epochs=100,\n",
    "  validation_data=(X_test, Y_test),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335/335 [==============================] - 0s 110us/sample - loss: 5.4033 - acc: 0.4896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.403338604542746, 0.48955223]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
