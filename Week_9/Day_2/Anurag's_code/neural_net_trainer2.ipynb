{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras import metrics\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class NeuralNetTrainer:\n",
    "    \n",
    "    '''\n",
    "    self.key_column <- user friendly key to the data.. Date in this case\n",
    "    self.features <- the fields we are interested in\n",
    "    self.dataset <- DF with selected features\n",
    "    self.raw <- DF with selected features and key field\n",
    "    self.scaled <- DF of scaled, series data. () \n",
    "    self.processed_data <- DF with key field and scaled  \n",
    "    self.time_steps_in <- Number of time steps in input\n",
    "    self.time_steps_out <- Number of time steps in output\n",
    "    self.train_X, self.train_Y, self.test_X, self.test_Y <- numpy arrays\n",
    "    self.x_scaler, self.y_scaler <- MinMax scalers\n",
    "    self.model <- the trained model \n",
    "    self.graph <- the trained graph\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    float_formatter = lambda x: \"%.2f\" % x\n",
    "    np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "    \n",
    "    def load_data(self, training_file='F:\\\\New folder\\\\project_stock\\\\cl1_oil_2000.csv', features=['close', 'macd', 'macd_signal'], key_column='date'):\n",
    "        \n",
    "        #load maybe called with different file.\n",
    "        #if hasattr(self, 'dataset') != True:\n",
    "        self.dataset = read_csv(training_file, header=0)\n",
    "        self.keys = self.dataset[key_column]\n",
    "        self.key_column = key_column\n",
    "        self.features = features\n",
    "        #print(self.dataset.columns.values.tolist())\n",
    "        #dataset.drop(['Unnamed: 0', 'date', 'open', 'high', 'low', 'volume', 'sma','ema','stochOSR.fastK','stochOSR.fastD','stochWPR', 'rsi','vwma' ,'trend','change'], axis=1, inplace=True)\n",
    "        print(\"Features Selected: \" + str(features))\n",
    "        # Selecting the ones I want\n",
    "        self.dataset = self.dataset[features]\n",
    "        self.raw = concat([self.keys, self.dataset], axis=1)\n",
    "        self.raw = self.raw.set_index(self.key_column)\n",
    "\n",
    "    def set_x_scaler(self, scaler):\n",
    "        self.x_scaler = scaler\n",
    "    \n",
    "    def set_y_scaler(self, scaler):\n",
    "        self.y_scaler = scaler\n",
    "    \n",
    "    def set_y(self, ycolumns):\n",
    "        self.y = ycolumns\n",
    "        \n",
    "    def scale_(self):\n",
    "        if hasattr(self, 'x_scaler') != True:\n",
    "            self.x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        if hasattr(self, 'y_scaler') != True:\n",
    "            self.y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        \n",
    "        self.scaled = DataFrame(data=self.x_scaler.fit_transform(self.dataset.values), columns=self.dataset.columns)\n",
    "        if hasattr(self, 'y'):\n",
    "            self.scaledy = DataFrame(data=self.y_scaler.fit_transform(self.dataset[self.y].values), columns=self.y)\n",
    "    \n",
    "    def one_hot_encode(self, column_to_encode):\n",
    "        data = self.dataset[column_to_encode]\n",
    "        values = array(data)\n",
    "        print(values)\n",
    "        # integer encode\n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_encoded = label_encoder.fit_transform(values)\n",
    "        print(integer_encoded)\n",
    "        # binary encode\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "        onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "        #print(onehot_encoded)\n",
    "        oh_df = DataFrame(data=onehot_encoded)\n",
    "        dataset2 = concat([self.dataset, oh_df], axis=1)\n",
    "        self.dataset = dataset2.drop([column_to_encode], axis=1)\n",
    "        \n",
    "        \n",
    "    # convert series to supervised learning\n",
    "    def series_to_supervised(self, n_in=1, n_out=1, dropnan=True):\n",
    "        data = self.scaled.values\n",
    "        col_names = self.scaled.columns.values.tolist()\n",
    "        n_vars = 1 if type(data) is list else data.shape[1]\n",
    "        df = DataFrame(data)\n",
    "        #print(df.head())\n",
    "        cols, names = list(), list()\n",
    "        # input sequence (t-n, ... t-1)\n",
    "        for i in range(n_in, 0, -1):\n",
    "            cols.append(df.shift(i))\n",
    "            names += [col_names[j] +'(t-%d)' % (i) for j in range(n_vars)]\n",
    "        # forecast sequence (t, t+1, ... t+n)\n",
    "        for i in range(0, n_out):\n",
    "            cols.append(df.shift(-i))\n",
    "            if i == 0:\n",
    "                names += [col_names[j] +'(t)' for j in range(n_vars)]\n",
    "            else:\n",
    "                names += [col_names[j] +'(t+%d)' % (i) for j in range(n_vars)]\n",
    "        # put it all together\n",
    "        agg = concat(cols, axis=1)\n",
    "        agg.columns = names\n",
    "        # drop rows with NaN values\n",
    "        self.processed_data = concat([self.keys, agg], axis=1)\n",
    "        self.processed_data = self.processed_data.set_index(self.key_column)\n",
    "        \n",
    "        #print(self.processed_data.head())\n",
    "        if dropnan:\n",
    "            agg.dropna(inplace=True)\n",
    "            self.processed_data.dropna(inplace=True)\n",
    "        return agg\n",
    "\n",
    "    def create_time_series(self, n_in=1, n_out=1):\n",
    "        self.time_steps_in = n_in\n",
    "        self.time_steps_out = n_out\n",
    "        print('Time Steps In: ' + str(n_in))\n",
    "        print('Time Steps Out: ' + str(n_out))\n",
    "        self.scale_()\n",
    "        self.scaled = DataFrame(data=self.series_to_supervised(n_in, n_out))\n",
    "        \n",
    "    \n",
    "    def get_predict_cols(self):\n",
    "        n_vars = len(self.y)\n",
    "        names = list()\n",
    "        for i in range(0, self.time_steps_out):\n",
    "            if i == 0:\n",
    "                names += [self.y[j] +'(t)' for j in range(n_vars)]\n",
    "            else:\n",
    "                names += [self.y[j] +'(t+%d)' % (i) for j in range(n_vars)]\n",
    "        return names\n",
    "    \n",
    "    def get_ts_forward_pattern(self):\n",
    "        names = list()\n",
    "        for i in range(0, self.time_steps_out):\n",
    "            if i == 0:\n",
    "                names += ['(t)']\n",
    "            else:\n",
    "                names += ['(t+%d)' % (i)]\n",
    "        return names\n",
    "    \n",
    "    def get_train_cols(self):\n",
    "        all_cols = trainer.scaled.columns.values.tolist()\n",
    "        predict_cols = self.get_predict_cols()\n",
    "        \n",
    "        for col in predict_cols:\n",
    "            all_cols.remove(col) \n",
    "        \n",
    "        ts_fwd_patterns = self.get_ts_forward_pattern()\n",
    "\n",
    "        for pattern in ts_fwd_patterns:\n",
    "            all_cols = list(filter(lambda coll: pattern not in coll, all_cols))\n",
    "        return all_cols\n",
    "    \n",
    "    def vertical_split(self):      \n",
    "        if hasattr(self, 'y'):\n",
    "            self.Y = self.scaled.loc[:, self.get_predict_cols()]\n",
    "            self.X = self.scaled.loc[:, self.get_train_cols()]           \n",
    "            print('Training Columns: ' + str(self.X.columns.values.tolist()))\n",
    "            print('Prediction Columns: ' + str(self.Y.columns.values.tolist()))\n",
    "            #print(self.Y.head())\n",
    "        else:\n",
    "            no_train_cols = len(self.dataset.columns.values.tolist()) * self.time_steps_in\n",
    "            self.X = self.scaled.iloc[:, 0:no_train_cols]\n",
    "            self.Y = self.scaled.iloc[:, no_train_cols:]\n",
    "            print(self.Y.head())\n",
    "        \n",
    "\n",
    "    def horizontal_split(self, offset, n_train_records, n_test_records=-1):\n",
    "        start_train = offset\n",
    "        end_train = offset + n_train_records\n",
    "        start_test = end_train + 1\n",
    "        end_test = start_test + n_test_records\n",
    "        \n",
    "        #print(self.dataset[self.key_column][])\n",
    "        self.train_X = self.X.loc[start_train:end_train, :].values        \n",
    "        self.train_Y = self.Y.loc[start_train:end_train, :].values\n",
    "        if n_test_records != -1:\n",
    "            self.test_X = self.X.loc[start_test:end_test, :].values\n",
    "            self.test_Y = self.Y.loc[start_test:end_test, :].values\n",
    "        else:\n",
    "            self.test_X = self.X.loc[start_test:, :].values\n",
    "            self.test_Y = self.Y.loc[start_test:, :].values\n",
    "        self.reshape()\n",
    "        \n",
    "    def reshape(self):\n",
    "         # reshape input to be 3D [samples, timesteps, features]\n",
    "        no_features = len(self.dataset.columns.values.tolist())\n",
    "        self.train_X = self.train_X.reshape((self.train_X.shape[0], self.time_steps_in, no_features))\n",
    "        self.test_X = self.test_X.reshape((self.test_X.shape[0], self.time_steps_in, no_features))\n",
    "        \n",
    "    def split_by_factor_linear(self, train_factor):\n",
    "        self.vertical_split()\n",
    "        n_train_records = int(self.X.shape[0] * train_factor)\n",
    "        self.horizontal_split(0, n_train_records)\n",
    "    \n",
    "    def split_by_factor_random(self, train_factor):\n",
    "        #horizontal followed by vertical!! for this case\n",
    "        self.vertical_split()\n",
    "        self.train_X, self.test_X, self.train_Y, self.test_Y = train_test_split(self.X.values, self.Y.values, test_size=(1-train_factor))\n",
    "        #print(\"type------->\", str(train_X.columns.values.tolist()))\n",
    "        self.reshape()\n",
    "        \n",
    "    def split_by_count(self, n_train_records, n_test_records, offset=0):\n",
    "        self.vertical_split()\n",
    "        self.horizontal_split(offset, n_train_records, n_test_records)        \n",
    "        \n",
    "    def split_by_key(self, train_start_key, test_start_key):\n",
    "        self.vertical_split()\n",
    "        train_start_ind =  self.processed_data.index.get_loc(train_start_key)\n",
    "        test_start_ind =  self.processed_data.index.get_loc(test_start_key)\n",
    "        print('train_start_ind ', train_start_ind)\n",
    "        print('test_start_ind', test_start_ind)\n",
    "        n_test_records = self.processed_data.shape[0] - test_start_ind\n",
    "        if train_start_ind == 0:\n",
    "            offset = 0\n",
    "        else:\n",
    "            offset = train_start_ind - 1\n",
    "        self.horizontal_split(offset, (test_start_ind - train_start_ind), n_test_records)\n",
    "            \n",
    "    def set_model_builder(self, mb):\n",
    "        self.model_builder = mb\n",
    "        \n",
    "    def build(self, epochs=12, batch_size=18, model_file='F:\\\\New folder\\\\project_stock\\\\cl1_oil_model.h5'):\n",
    "        self.model = self.model_builder(self)\n",
    "        self.model.fit(self.train_X, self.train_Y, epochs=epochs, batch_size=batch_size, validation_data=(self.test_X, self.test_Y))\n",
    "        self.save()\n",
    "    \n",
    "    def retrain(self, epochs=12, batch_size=18, model_file='F:\\\\New folder\\\\project_stock\\\\cl1_oil_model.h5'):\n",
    "        if not hasattr(self, 'model'):\n",
    "            self.model = self.model_builder(self)\n",
    "        self.model.fit(self.train_X, self.train_Y, epochs=epochs, batch_size=batch_size, validation_data=(self.test_X, self.test_Y))\n",
    "        self.save()\n",
    "        \n",
    "    def save(self, model_file='F:\\\\New folder\\\\project_stock\\\\cl1_oil_model.h5'):\n",
    "        if hasattr(self, 'model'):\n",
    "            self.model.save(model_file)\n",
    "    \n",
    "    def load_trained_model(self, file='F:\\\\New folder\\\\project_stock\\\\cl1_oil_model.h5'):\n",
    "        model = load_model(file)\n",
    "        #https://github.com/fchollet/keras/issues/2397\n",
    "        model._make_predict_function()\n",
    "        self.graph = tf.get_default_graph()\n",
    "        self.model = model\n",
    "        \n",
    "    def get_reshaped_data(self, index, should_print=True):\n",
    "        df = self.processed_data.iloc[index:index+1, :]      \n",
    "        if hasattr(self, 'y'):\n",
    "            y = df[self.get_predict_cols()]\n",
    "            x = df[self.get_train_cols()]\n",
    "        else:               \n",
    "            no_train_cols = len(self.dataset.columns.values.tolist()) * self.time_steps_in\n",
    "            x = df.iloc[:, 0:no_train_cols]\n",
    "            y = df[no_train_cols:]\n",
    "           \n",
    "        x = x.values.reshape(1, self.time_steps_in, len(self.dataset.columns.values.tolist()))[0]\n",
    "        \n",
    "        #trans_input = self.x_scaler.inverse_transform(x)\n",
    "        if should_print:\n",
    "            print(\"<-----------Reshaped Data------------->\")\n",
    "            #print(\"{:0.2f}\".format(x) for x in trans_input)\n",
    "            print(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def predict(self, td):\n",
    "        with self.graph.as_default():\n",
    "            yhat = self.model.predict(td)\n",
    "        if hasattr(self, 'y'):\n",
    "            inv_yhat = self.y_scaler.inverse_transform(yhat)\n",
    "        else:\n",
    "            inv_yhat = self.x_scaler.inverse_transform(yhat)\n",
    "        return inv_yhat\n",
    " \n",
    "    def make_prediction(self, key):\n",
    "        \n",
    "        if not hasattr(self, 'model') or not hasattr(self, 'graph'):\n",
    "            self.load_trained_model()                \n",
    "        ind = self.processed_data.index.get_loc(key)\n",
    "        td = self.get_reshaped_data(ind)\n",
    "        return self.predict(td)\n",
    "\n",
    "    def get_raw_data(self, key, num_record=1, offset=-1, should_print=True):\n",
    "        ind =  self.raw.index.get_loc(key)\n",
    "        raw_data = self.raw.iloc[(ind+offset):((ind+offset) + num_record), :]\n",
    "        if should_print:\n",
    "            print('<---Raw Data[Key:%s, num_records:%d, offset:%d] -------->' % (key, num_record, offset))\n",
    "            print(raw_data)\n",
    "        return raw_data\n",
    "  \n",
    "    \n",
    "    def get_processed_data(self, key, transform=True, should_print=True):\n",
    "        ind =  self.processed_data.index.get_loc(key)\n",
    "        x = self.get_reshaped_data(ind)\n",
    "        if transform:\n",
    "            x = self.x_scaler.inverse_transform(x)\n",
    "       \n",
    "        if should_print:\n",
    "            print(\"<-----------Prediction Input------------->\")\n",
    "            print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\shilpi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-555979e6b349>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNetTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_model_builder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'F:\\\\New folder\\\\project_stock\\\\cl1_oil_2000.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'close'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'macd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'macd_signal'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'close'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'macd'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'macd_signal'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_time_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-02f1afc29c5a>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(self, training_file, features, key_column)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m#if hasattr(self, 'dataset') != True:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey_column\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_column\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shilpi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shilpi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "def model_builder(trainer):\n",
    "    model = Sequential()  \n",
    "    #model.add(Flatten())\n",
    "    model.add(LSTM(2, input_shape=trainer.train_X.shape[1:], return_sequences=True, activation='relu'))\n",
    "    model.add(LSTM(2, input_shape=trainer.train_X.shape[1:], return_sequences=False, activation='relu'))\n",
    "    #model.add(Dense(4))\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "mb = model_builder\n",
    "trainer = NeuralNetTrainer()\n",
    "trainer.set_model_builder(mb)\n",
    "trainer.load_data('F:\\\\New folder\\\\project_stock\\\\cl1_oil_2000.csv', features=['close','macd', 'macd_signal'])\n",
    "trainer.set_y(['close','macd','macd_signal'])\n",
    "trainer.create_time_series(2, 1)\n",
    "trainer.split_by_factor_random(.90)\n",
    "#trainer.split_by_factor_linear(.95)\n",
    "#trainer.split_by_key(train_start_key='1/11/2000', test_start_key='9/1/2017')\n",
    "\n",
    "trainer.build(epochs=50, batch_size=11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedModel = load_model('F:\\\\New folder\\\\project_stock\\\\cl1_oil_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=savedModel.predict(trainer.test_X)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inversePred=trainer.y_scaler.inverse_transform(pred)\n",
    "inversePred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1 = NeuralNetTrainer()\n",
    "trainer1.set_model_builder(mb)\n",
    "trainer1.load_data('F:\\\\New folder\\\\project_stock\\\\cl1_oil_2000.csv', features=['close','macd', 'macd_signal'])\n",
    "trainer1.set_y(['close','macd','macd_signal'])\n",
    "trainer1.create_time_series(3, 1)\n",
    "trainer1.split_by_factor_linear(.90)\n",
    "#trainer.split_by_factor_linear(.95)\n",
    "#trainer.split_by_key(train_start_key='1/11/2000', test_start_key='9/1/2017')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1=savedModel.predict(trainer1.test_X)\n",
    "pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inversePred1=trainer.y_scaler.inverse_transform(pred1)\n",
    "inversePred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual=trainer.dataset['close']\n",
    "actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual=trainer1.test_Y\n",
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actualTestY=trainer1.y_scaler.inverse_transform(actual)\n",
    "actualTestY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualPlot=actualTestY[:,0:1]\n",
    "predictPlot=inversePred1[:,0:1]\n",
    "print(actualPlot.shape)\n",
    "print(predictPlot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "t = np.arange(0, 446, 1)\n",
    "s1=actualPlot\n",
    "s2=predictPlot\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "p1=plt.plot(t, s1,label='Actual')\n",
    "p2=plt.plot(t,s2,label='Predicted')\n",
    "\n",
    "\n",
    "#plt.subplot(212)\n",
    "#plt.plot(t, 2*s1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " mean_absolute_percentage_error(actualPlot,predictPlot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(actualPlot, predictPlot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(actualPlot,predictPlot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rms = sqrt(mean_squared_error(actualPlot, predictPlot))\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
